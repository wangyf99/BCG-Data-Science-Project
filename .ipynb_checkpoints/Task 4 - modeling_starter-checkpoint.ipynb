{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4noH7q4USKQ"
   },
   "source": [
    "# Feature Engineering and Modelling\n",
    "\n",
    "---\n",
    "\n",
    "1. Import packages\n",
    "2. Load data\n",
    "3. Modelling\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NwE6osQpUSKS"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cm3WmjAZUSKT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Shows plots in jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plot style\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed directory to: /Users/yifanwang\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.expanduser('~'))  # Changes to your home directory\n",
    "print(\"Changed directory to:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sewogUFaUSKU"
   },
   "source": [
    "---\n",
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "oaIJfXJjUSKU",
    "outputId": "c218e74d-f4bb-4150-b1e3-22d38d83fc07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cons_12m</th>\n",
       "      <th>cons_gas_12m</th>\n",
       "      <th>cons_last_month</th>\n",
       "      <th>forecast_cons_12m</th>\n",
       "      <th>forecast_discount_energy</th>\n",
       "      <th>forecast_meter_rent_12m</th>\n",
       "      <th>forecast_price_energy_off_peak</th>\n",
       "      <th>forecast_price_energy_peak</th>\n",
       "      <th>forecast_price_pow_off_peak</th>\n",
       "      <th>...</th>\n",
       "      <th>months_modif_prod</th>\n",
       "      <th>months_renewal</th>\n",
       "      <th>channel_MISSING</th>\n",
       "      <th>channel_ewpakwlliwisiwduibdlfmalxowmwpci</th>\n",
       "      <th>channel_foosdfpfkusacimwkcsosbicdxkicaua</th>\n",
       "      <th>channel_lmkebamcaaclubfxadlmueccxoimlema</th>\n",
       "      <th>channel_usilxuppasemubllopkaafesmlibmsdf</th>\n",
       "      <th>origin_up_kamkkxfxxuwbdslkwifmmcsiusiuosws</th>\n",
       "      <th>origin_up_ldkssxwpmemidmecebumciepifcamkci</th>\n",
       "      <th>origin_up_lxidpiddsbxsbosboudacockeimpuepw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24011ae4ebbe3035111d65fa7c15bc57</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.739944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444045</td>\n",
       "      <td>0.114481</td>\n",
       "      <td>0.098142</td>\n",
       "      <td>40.606701</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d29c2c54acc38ff3c0614d0a653813dd</td>\n",
       "      <td>3.668479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.280920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.237292</td>\n",
       "      <td>0.145711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.311378</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>764c75f661154dac3a6c254cd082ea7d</td>\n",
       "      <td>2.736397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.689841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.599009</td>\n",
       "      <td>0.165794</td>\n",
       "      <td>0.087899</td>\n",
       "      <td>44.311378</td>\n",
       "      <td>...</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bba03439a292a1e166f80264c16191cb</td>\n",
       "      <td>3.200029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.382089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.318689</td>\n",
       "      <td>0.146694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.311378</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149d57cf92fc41cf94415803a877cb4b</td>\n",
       "      <td>3.646011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.721811</td>\n",
       "      <td>2.650065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.122969</td>\n",
       "      <td>0.116900</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>40.606701</td>\n",
       "      <td>...</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  cons_12m  cons_gas_12m  cons_last_month  \\\n",
       "0  24011ae4ebbe3035111d65fa7c15bc57  0.000000      4.739944         0.000000   \n",
       "1  d29c2c54acc38ff3c0614d0a653813dd  3.668479      0.000000         0.000000   \n",
       "2  764c75f661154dac3a6c254cd082ea7d  2.736397      0.000000         0.000000   \n",
       "3  bba03439a292a1e166f80264c16191cb  3.200029      0.000000         0.000000   \n",
       "4  149d57cf92fc41cf94415803a877cb4b  3.646011      0.000000         2.721811   \n",
       "\n",
       "   forecast_cons_12m  forecast_discount_energy  forecast_meter_rent_12m  \\\n",
       "0           0.000000                       0.0                 0.444045   \n",
       "1           2.280920                       0.0                 1.237292   \n",
       "2           1.689841                       0.0                 1.599009   \n",
       "3           2.382089                       0.0                 1.318689   \n",
       "4           2.650065                       0.0                 2.122969   \n",
       "\n",
       "   forecast_price_energy_off_peak  forecast_price_energy_peak  \\\n",
       "0                        0.114481                    0.098142   \n",
       "1                        0.145711                    0.000000   \n",
       "2                        0.165794                    0.087899   \n",
       "3                        0.146694                    0.000000   \n",
       "4                        0.116900                    0.100015   \n",
       "\n",
       "   forecast_price_pow_off_peak  ...  months_modif_prod  months_renewal  \\\n",
       "0                    40.606701  ...                  2               6   \n",
       "1                    44.311378  ...                 76               4   \n",
       "2                    44.311378  ...                 68               8   \n",
       "3                    44.311378  ...                 69               9   \n",
       "4                    40.606701  ...                 71               9   \n",
       "\n",
       "   channel_MISSING  channel_ewpakwlliwisiwduibdlfmalxowmwpci  \\\n",
       "0                0                                         0   \n",
       "1                1                                         0   \n",
       "2                0                                         0   \n",
       "3                0                                         0   \n",
       "4                1                                         0   \n",
       "\n",
       "   channel_foosdfpfkusacimwkcsosbicdxkicaua  \\\n",
       "0                                         1   \n",
       "1                                         0   \n",
       "2                                         1   \n",
       "3                                         0   \n",
       "4                                         0   \n",
       "\n",
       "   channel_lmkebamcaaclubfxadlmueccxoimlema  \\\n",
       "0                                         0   \n",
       "1                                         0   \n",
       "2                                         0   \n",
       "3                                         1   \n",
       "4                                         0   \n",
       "\n",
       "   channel_usilxuppasemubllopkaafesmlibmsdf  \\\n",
       "0                                         0   \n",
       "1                                         0   \n",
       "2                                         0   \n",
       "3                                         0   \n",
       "4                                         0   \n",
       "\n",
       "   origin_up_kamkkxfxxuwbdslkwifmmcsiusiuosws  \\\n",
       "0                                           0   \n",
       "1                                           1   \n",
       "2                                           1   \n",
       "3                                           1   \n",
       "4                                           1   \n",
       "\n",
       "   origin_up_ldkssxwpmemidmecebumciepifcamkci  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "\n",
       "   origin_up_lxidpiddsbxsbosboudacockeimpuepw  \n",
       "0                                           1  \n",
       "1                                           0  \n",
       "2                                           0  \n",
       "3                                           0  \n",
       "4                                           0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Documents/GitHub/BCG-Data-Science-Project/data_for_predictions.csv')\n",
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2gjvbtCUSKV"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Modelling\n",
    "\n",
    "We now have a dataset containing features that we have engineered and we are ready to start training a predictive model. Remember, we only need to focus on training a `Random Forest` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cPHZWHC8USKV"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PitUvSFhUSKV"
   },
   "source": [
    "### Data sampling\n",
    "\n",
    "The first thing we want to do is split our dataset into training and test samples. The reason why we do this, is so that we can simulate a real life situation by generating predictions for our test sample, without showing the predictive model these data points. This gives us the ability to see how well our model is able to generalise to new data, which is critical.\n",
    "\n",
    "A typical % to dedicate to testing is between 20-30, for this example we will use a 75-25% split between train and test respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dstIVhBnUSKW",
    "outputId": "fdc82a65-36c0-4229-d729-161989dccda7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14606, 61)\n",
      "(14606,)\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of our data\n",
    "train_df = df.copy()\n",
    "\n",
    "# Separate target variable from independent variables\n",
    "y = df['churn']\n",
    "X = df.drop(columns=['id', 'churn'])\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ifim4p1WUSKW",
    "outputId": "d689475d-555c-48d0-a0be-a0fbe812ab47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10954, 61)\n",
      "(10954,)\n",
      "(3652, 61)\n",
      "(3652,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22A_oe_PUSKX"
   },
   "source": [
    "### Model training\n",
    "\n",
    "Once again, we are using a `Random Forest` classifier in this example. A Random Forest sits within the category of `ensemble` algorithms because internally the `Forest` refers to a collection of `Decision Trees` which are tree-based learning algorithms. As the data scientist, you can control how large the forest is (that is, how many decision trees you want to include).\n",
    "\n",
    "The reason why an `ensemble` algorithm is powerful is because of the laws of averaging, weak learners and the central limit theorem. If we take a single decision tree and give it a sample of data and some parameters, it will learn patterns from the data. It may be overfit or it may be underfit, but that is now our only hope, that single algorithm. \n",
    "\n",
    "With `ensemble` methods, instead of banking on 1 single trained model, we can train 1000's of decision trees, all using different splits of the data and learning different patterns. It would be like asking 1000 people to all learn how to code. You would end up with 1000 people with different answers, methods and styles! The weak learner notion applies here too, it has been found that if you train your learners not to overfit, but to learn weak patterns within the data and you have a lot of these weak learners, together they come together to form a highly predictive pool of knowledge! This is a real life application of many brains are better than 1.\n",
    "\n",
    "Now instead of relying on 1 single decision tree for prediction, the random forest puts it to the overall views of the entire collection of decision trees. Some ensemble algorithms using a voting approach to decide which prediction is best, others using averaging. \n",
    "\n",
    "As we increase the number of learners, the idea is that the random forest's performance should converge to its best possible solution.\n",
    "\n",
    "Some additional advantages of the random forest classifier include:\n",
    "\n",
    "- The random forest uses a rule-based approach instead of a distance calculation and so features do not need to be scaled\n",
    "- It is able to handle non-linear parameters better than linear based models\n",
    "\n",
    "On the flip side, some disadvantages of the random forest classifier include:\n",
    "\n",
    "- The computational power needed to train a random forest on a large dataset is high, since we need to build a whole ensemble of estimators.\n",
    "- Training time can be longer due to the increased complexity and size of thee ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_safe_tags' from 'sklearn.utils._tags' (/opt/miniconda3/envs/myenv/lib/python3.12/site-packages/sklearn/utils/_tags.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomOverSampler, SMOTE, ADASYN\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munder_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomUnderSampler\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/myenv/lib/python3.12/site-packages/imblearn/__init__.py:52\u001b[39m\n\u001b[32m     48\u001b[39m     sys.stderr.write(\u001b[33m\"\u001b[39m\u001b[33mPartial import of imblearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     53\u001b[39m         combine,\n\u001b[32m     54\u001b[39m         ensemble,\n\u001b[32m     55\u001b[39m         exceptions,\n\u001b[32m     56\u001b[39m         metrics,\n\u001b[32m     57\u001b[39m         over_sampling,\n\u001b[32m     58\u001b[39m         pipeline,\n\u001b[32m     59\u001b[39m         tensorflow,\n\u001b[32m     60\u001b[39m         under_sampling,\n\u001b[32m     61\u001b[39m         utils,\n\u001b[32m     62\u001b[39m     )\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionSampler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/myenv/lib/python3.12/site-packages/imblearn/ensemble/__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThe :mod:`imblearn.ensemble` module include methods generating\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03munder-sampled subsets combined inside an ensemble.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bagging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BalancedBaggingClassifier\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_easy_ensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EasyEnsembleClassifier\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_forest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BalancedRandomForestClassifier\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_weight_boosting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RUSBoostClassifier\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/myenv/lib/python3.12/site-packages/imblearn/ensemble/_easy_ensemble.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _partition_estimators\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, StrOptions\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _safe_tags\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetaestimators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m available_if\n",
      "\u001b[31mImportError\u001b[39m: cannot import name '_safe_tags' from 'sklearn.utils._tags' (/opt/miniconda3/envs/myenv/lib/python3.12/site-packages/sklearn/utils/_tags.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "\n",
    "# Prepare X and y\n",
    "X = data.drop(columns=['Unnamed: 0', 'id', 'churn'])\n",
    "y = data['churn']\n",
    "\n",
    "# Define balancing methods\n",
    "balancers = {\n",
    "    \"RandomOverSampler\": RandomOverSampler(random_state=42),\n",
    "    \"SMOTE\": SMOTE(random_state=42),\n",
    "    \"ADASYN\": ADASYN(random_state=42),\n",
    "    \"RandomUnderSampler\": RandomUnderSampler(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, balancer in balancers.items():\n",
    "    # Balance the data\n",
    "    X_bal, y_bal = balancer.fit_resample(X, y)\n",
    "    \n",
    "    # Train/test split evaluation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_bal, y_bal, test_size=0.2, random_state=42, stratify=y_bal\n",
    "    )\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=42, n_estimators=200)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    split_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Cross-validation evaluation\n",
    "    cv_scores = cross_val_score(model, X_bal, y_bal, cv=5, scoring='f1')\n",
    "    cv_mean = np.mean(cv_scores)\n",
    "    \n",
    "    results.append({\n",
    "        \"method\": name,\n",
    "        **split_metrics,\n",
    "        \"cv_f1_mean\": cv_mean\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"cv_f1_mean\", ascending=False)\n",
    "import caas_jupyter_tools\n",
    "caas_jupyter_tools.display_dataframe_to_user(name=\"Balancing Method Comparison\", dataframe=results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 23:18:58,079 - INFO - Successfully loaded data with shape: (14606, 64)\n",
      "2025-08-09 23:18:58,086 - INFO - Removed unnamed index column\n",
      "2025-08-09 23:18:58,089 - INFO - Features shape: (14606, 61), Target shape: (14606,)\n",
      "2025-08-09 23:18:58,091 - INFO - Target distribution:\n",
      "churn\n",
      "0    13187\n",
      "1     1419\n",
      "Name: count, dtype: int64\n",
      "2025-08-09 23:18:58,092 - INFO - Testing balancing method: none\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing balancing methods...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 23:19:10,778 - INFO - Method none - F1: 0.0209\n",
      "2025-08-09 23:19:10,779 - INFO - Testing balancing method: oversample\n",
      "2025-08-09 23:19:10,833 - INFO - Balanced data using oversample: (26374, 61)\n",
      "2025-08-09 23:19:10,835 - INFO - New target distribution:\n",
      "churn\n",
      "0    13187\n",
      "1    13187\n",
      "Name: count, dtype: int64\n",
      "2025-08-09 23:19:30,214 - INFO - Method oversample - F1: 0.8376\n",
      "2025-08-09 23:19:30,215 - INFO - Testing balancing method: undersample\n",
      "2025-08-09 23:19:30,231 - INFO - Balanced data using undersample: (2838, 61)\n",
      "2025-08-09 23:19:30,232 - INFO - New target distribution:\n",
      "churn\n",
      "0    1419\n",
      "1    1419\n",
      "Name: count, dtype: int64\n",
      "2025-08-09 23:19:33,003 - INFO - Method undersample - F1: 0.6082\n",
      "2025-08-09 23:19:33,009 - INFO - Best balancing method: oversample\n",
      "2025-08-09 23:19:33,055 - INFO - Balanced data using oversample: (26374, 61)\n",
      "2025-08-09 23:19:33,057 - INFO - New target distribution:\n",
      "churn\n",
      "0    13187\n",
      "1    13187\n",
      "Name: count, dtype: int64\n",
      "2025-08-09 23:19:33,080 - INFO - Train set: (19780, 61), Test set: (6594, 61)\n",
      "2025-08-09 23:19:33,081 - INFO - Train target distribution:\n",
      "churn\n",
      "1    9890\n",
      "0    9890\n",
      "Name: count, dtype: int64\n",
      "2025-08-09 23:19:33,084 - INFO - Training final Random Forest model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balancing Method Comparison:\n",
      "        method  accuracy  precision  recall      f1  cv_f1_mean  cv_f1_std\n",
      "0         none    0.9038     1.0000  0.0106  0.0209      0.0181     0.0055\n",
      "1   oversample    0.8347     0.8231  0.8525  0.8376      0.8385     0.0044\n",
      "2  undersample    0.5986     0.5940  0.6232  0.6082      0.6038     0.0227\n",
      "\n",
      "Training final model with oversample balancing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 23:19:41,238 - INFO - Final model training completed successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL EVALUATION RESULTS ===\n",
      "Accuracy:  0.9598\n",
      "Precision: 0.9420\n",
      "Recall:    0.9800\n",
      "F1-Score:  0.9606\n",
      "Best Balancing Method: oversample\n",
      "\n",
      "=== CONFUSION MATRIX ===\n",
      "[[3098  199]\n",
      " [  66 3231]]\n",
      "\n",
      "=== DETAILED CLASSIFICATION REPORT ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96      3297\n",
      "           1       0.94      0.98      0.96      3297\n",
      "\n",
      "    accuracy                           0.96      6594\n",
      "   macro avg       0.96      0.96      0.96      6594\n",
      "weighted avg       0.96      0.96      0.96      6594\n",
      "\n",
      "\n",
      "=== TOP 10 FEATURE IMPORTANCES ===\n",
      "                        feature  importance\n",
      "12           margin_net_pow_ele    0.058319\n",
      "11         margin_gross_pow_ele    0.055808\n",
      "0                      cons_12m    0.054146\n",
      "5       forecast_meter_rent_12m    0.043959\n",
      "2               cons_last_month    0.039856\n",
      "14                   net_margin    0.039397\n",
      "3             forecast_cons_12m    0.037870\n",
      "49                 months_activ    0.033377\n",
      "16  var_year_price_off_peak_var    0.031940\n",
      "15                      pow_max    0.030156\n",
      "\n",
      "üéâ Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced ML Pipeline with Comprehensive Error Handling\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                           f1_score, classification_report, confusion_matrix)\n",
    "from sklearn.utils import resample\n",
    "import logging\n",
    "\n",
    "# Set up logging for error tracking\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MLPipelineWithErrorHandling:\n",
    "    \"\"\"\n",
    "    A robust ML pipeline with comprehensive error handling for churn prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.model = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.best_balancing_method = None\n",
    "        self.results = {}\n",
    "        \n",
    "    def safe_data_loading(self, filepath):\n",
    "        \"\"\"Safely load data with error handling\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "            logger.info(f\"Successfully loaded data with shape: {df.shape}\")\n",
    "            \n",
    "            # Handle common data issues\n",
    "            if \"Unnamed: 0\" in df.columns:\n",
    "                df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "                logger.info(\"Removed unnamed index column\")\n",
    "                \n",
    "            return df\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {filepath}\")\n",
    "            raise\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logger.error(\"CSV file is empty\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error loading data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def safe_feature_target_split(self, df, target_col='churn', id_col='id'):\n",
    "        \"\"\"Safely split features and target with validation\"\"\"\n",
    "        try:\n",
    "            # Validate required columns exist\n",
    "            if target_col not in df.columns:\n",
    "                raise ValueError(f\"Target column '{target_col}' not found in dataframe\")\n",
    "            \n",
    "            # Prepare features and target\n",
    "            cols_to_drop = [col for col in [id_col, target_col] if col in df.columns]\n",
    "            X = df.drop(columns=cols_to_drop)\n",
    "            y = df[target_col]\n",
    "            \n",
    "            # Validate data\n",
    "            if X.empty or y.empty:\n",
    "                raise ValueError(\"Features or target is empty\")\n",
    "                \n",
    "            if X.isnull().any().any():\n",
    "                logger.warning(\"Missing values found in features - consider handling them\")\n",
    "                \n",
    "            logger.info(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "            logger.info(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "            \n",
    "            return X, y\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in feature-target split: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def safe_train_test_split(self, X, y, test_size=0.25):\n",
    "        \"\"\"Safely perform train-test split with validation\"\"\"\n",
    "        try:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=self.random_state, \n",
    "                stratify=y\n",
    "            )\n",
    "            \n",
    "            self.X_train, self.X_test = X_train, X_test\n",
    "            self.y_train, self.y_test = y_train, y_test\n",
    "            \n",
    "            logger.info(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "            logger.info(f\"Train target distribution:\\n{y_train.value_counts()}\")\n",
    "            \n",
    "            return X_train, X_test, y_train, y_test\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in train-test split: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def manual_balance_data(self, X, y, method='oversample'):\n",
    "        \"\"\"\n",
    "        Manual data balancing without imblearn dependency\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df_combined = pd.concat([X, y], axis=1)\n",
    "            \n",
    "            if method == 'oversample':\n",
    "                # Oversample minority class\n",
    "                class_counts = y.value_counts()\n",
    "                majority_class = class_counts.idxmax()\n",
    "                minority_class = class_counts.idxmin()\n",
    "                max_count = class_counts.max()\n",
    "                \n",
    "                majority_df = df_combined[df_combined[y.name] == majority_class]\n",
    "                minority_df = df_combined[df_combined[y.name] == minority_class]\n",
    "                \n",
    "                # Oversample minority class\n",
    "                minority_oversampled = resample(\n",
    "                    minority_df, replace=True, n_samples=max_count, \n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "                \n",
    "                balanced_df = pd.concat([majority_df, minority_oversampled])\n",
    "                \n",
    "            elif method == 'undersample':\n",
    "                # Undersample majority class\n",
    "                class_counts = y.value_counts()\n",
    "                majority_class = class_counts.idxmax()\n",
    "                minority_class = class_counts.idxmin()\n",
    "                min_count = class_counts.min()\n",
    "                \n",
    "                majority_df = df_combined[df_combined[y.name] == majority_class]\n",
    "                minority_df = df_combined[df_combined[y.name] == minority_class]\n",
    "                \n",
    "                # Undersample majority class\n",
    "                majority_undersampled = resample(\n",
    "                    majority_df, replace=False, n_samples=min_count, \n",
    "                    random_state=self.random_state\n",
    "                )\n",
    "                \n",
    "                balanced_df = pd.concat([majority_undersampled, minority_df])\n",
    "            \n",
    "            else:\n",
    "                balanced_df = df_combined\n",
    "            \n",
    "            # Shuffle the balanced dataset\n",
    "            balanced_df = balanced_df.sample(frac=1, random_state=self.random_state).reset_index(drop=True)\n",
    "            \n",
    "            X_balanced = balanced_df.drop(columns=[y.name])\n",
    "            y_balanced = balanced_df[y.name]\n",
    "            \n",
    "            logger.info(f\"Balanced data using {method}: {X_balanced.shape}\")\n",
    "            logger.info(f\"New target distribution:\\n{y_balanced.value_counts()}\")\n",
    "            \n",
    "            return X_balanced, y_balanced\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in data balancing: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def compare_balancing_methods(self, X, y):\n",
    "        \"\"\"Compare different balancing methods\"\"\"\n",
    "        methods = ['none', 'oversample', 'undersample']\n",
    "        results = []\n",
    "        \n",
    "        for method in methods:\n",
    "            try:\n",
    "                logger.info(f\"Testing balancing method: {method}\")\n",
    "                \n",
    "                if method == 'none':\n",
    "                    X_bal, y_bal = X, y\n",
    "                else:\n",
    "                    X_bal, y_bal = self.manual_balance_data(X, y, method)\n",
    "                \n",
    "                # Split balanced data\n",
    "                X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(\n",
    "                    X_bal, y_bal, test_size=0.2, random_state=self.random_state, \n",
    "                    stratify=y_bal\n",
    "                )\n",
    "                \n",
    "                # Train model\n",
    "                model = RandomForestClassifier(\n",
    "                    random_state=self.random_state, \n",
    "                    n_estimators=100,  # Reduced for faster training\n",
    "                    max_depth=10\n",
    "                )\n",
    "                model.fit(X_train_bal, y_train_bal)\n",
    "                y_pred = model.predict(X_test_bal)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = {\n",
    "                    'method': method,\n",
    "                    'accuracy': accuracy_score(y_test_bal, y_pred),\n",
    "                    'precision': precision_score(y_test_bal, y_pred, zero_division=0),\n",
    "                    'recall': recall_score(y_test_bal, y_pred, zero_division=0),\n",
    "                    'f1': f1_score(y_test_bal, y_pred, zero_division=0)\n",
    "                }\n",
    "                \n",
    "                # Cross-validation\n",
    "                try:\n",
    "                    cv_scores = cross_val_score(model, X_bal, y_bal, cv=5, scoring='f1')\n",
    "                    metrics['cv_f1_mean'] = np.mean(cv_scores)\n",
    "                    metrics['cv_f1_std'] = np.std(cv_scores)\n",
    "                except Exception as cv_e:\n",
    "                    logger.warning(f\"Cross-validation failed for {method}: {str(cv_e)}\")\n",
    "                    metrics['cv_f1_mean'] = metrics['f1']\n",
    "                    metrics['cv_f1_std'] = 0\n",
    "                \n",
    "                results.append(metrics)\n",
    "                logger.info(f\"Method {method} - F1: {metrics['f1']:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error testing method {method}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not results:\n",
    "            raise ValueError(\"No balancing methods completed successfully\")\n",
    "            \n",
    "        # Find best method\n",
    "        results_df = pd.DataFrame(results)\n",
    "        best_method = results_df.loc[results_df['cv_f1_mean'].idxmax(), 'method']\n",
    "        self.best_balancing_method = best_method\n",
    "        \n",
    "        logger.info(f\"Best balancing method: {best_method}\")\n",
    "        return results_df\n",
    "    \n",
    "    def train_final_model(self, X, y):\n",
    "        \"\"\"Train final model with best balancing method\"\"\"\n",
    "        try:\n",
    "            # Apply best balancing method\n",
    "            if self.best_balancing_method == 'none':\n",
    "                X_final, y_final = X, y\n",
    "            else:\n",
    "                X_final, y_final = self.manual_balance_data(X, y, self.best_balancing_method)\n",
    "            \n",
    "            # Final train-test split\n",
    "            X_train, X_test, y_train, y_test = self.safe_train_test_split(X_final, y_final)\n",
    "            \n",
    "            # Train final model with optimized parameters\n",
    "            self.model = RandomForestClassifier(\n",
    "                random_state=self.random_state,\n",
    "                n_estimators=200,\n",
    "                max_depth=15,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                class_weight='balanced'  # Additional balancing\n",
    "            )\n",
    "            \n",
    "            logger.info(\"Training final Random Forest model...\")\n",
    "            self.model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = self.model.predict(X_test)\n",
    "            y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Store results\n",
    "            self.results = {\n",
    "                'predictions': y_pred,\n",
    "                'probabilities': y_pred_proba,\n",
    "                'y_test': y_test,\n",
    "                'accuracy': accuracy_score(y_test, y_pred),\n",
    "                'precision': precision_score(y_test, y_pred),\n",
    "                'recall': recall_score(y_test, y_pred),\n",
    "                'f1': f1_score(y_test, y_pred),\n",
    "                'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "                'classification_report': classification_report(y_test, y_pred)\n",
    "            }\n",
    "            \n",
    "            logger.info(\"Final model training completed successfully!\")\n",
    "            return self.results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in final model training: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        if not self.results:\n",
    "            raise ValueError(\"Model must be trained first\")\n",
    "        \n",
    "        print(\"=== MODEL EVALUATION RESULTS ===\")\n",
    "        print(f\"Accuracy:  {self.results['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.results['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.results['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.results['f1']:.4f}\")\n",
    "        print(f\"Best Balancing Method: {self.best_balancing_method}\")\n",
    "        \n",
    "        print(\"\\n=== CONFUSION MATRIX ===\")\n",
    "        print(self.results['confusion_matrix'])\n",
    "        \n",
    "        print(\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
    "        print(self.results['classification_report'])\n",
    "        \n",
    "        # Feature importance\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': self.X_train.columns,\n",
    "                'importance': self.model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(\"\\n=== TOP 10 FEATURE IMPORTANCES ===\")\n",
    "            print(feature_importance.head(10))\n",
    "        \n",
    "        return self.results\n",
    "\n",
    "# Usage Example\n",
    "def run_ml_pipeline(data_path):\n",
    "    \"\"\"\n",
    "    Complete ML pipeline execution with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize pipeline\n",
    "        pipeline = MLPipelineWithErrorHandling(random_state=42)\n",
    "        \n",
    "        # Load and prepare data\n",
    "        df = pipeline.safe_data_loading(data_path)\n",
    "        X, y = pipeline.safe_feature_target_split(df)\n",
    "        \n",
    "        # Compare balancing methods\n",
    "        print(\"Comparing balancing methods...\")\n",
    "        balancing_results = pipeline.compare_balancing_methods(X, y)\n",
    "        print(\"\\nBalancing Method Comparison:\")\n",
    "        print(balancing_results.round(4))\n",
    "        \n",
    "        # Train final model\n",
    "        print(f\"\\nTraining final model with {pipeline.best_balancing_method} balancing...\")\n",
    "        final_results = pipeline.train_final_model(X, y)\n",
    "        \n",
    "        # Evaluate model\n",
    "        pipeline.evaluate_model()\n",
    "        \n",
    "        return pipeline, balancing_results, final_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# For your specific case - replace with your actual data path\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    data_path = 'Documents/GitHub/BCG-Data-Science-Project/data_for_predictions.csv'\n",
    "    \n",
    "    try:\n",
    "        pipeline, balancing_results, final_results = run_ml_pipeline(data_path)\n",
    "        print(\"\\nüéâ Pipeline completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pipeline failed: {str(e)}\")\n",
    "        print(\"Please check the data path and ensure all dependencies are installed.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "152bf6e7dc8ee53edb5af21dc1a8faeab7f134840808a94079ed98d91ece7e0c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
