


import warnings
warnings.filterwarnings("ignore", category=FutureWarning)


import pandas as pd
import numpy as np
import seaborn as sns
from datetime import datetime
import matplotlib.pyplot as plt

# Shows plots in jupyter notebook
%matplotlib inline

# Set plot style
sns.set(color_codes=True)


import os
os.chdir(os.path.expanduser('~'))  # Changes to your home directory
print("Changed directory to:", os.getcwd())






df = pd.read_csv('Documents/GitHub/BCG-Data-Science-Project/data_for_predictions.csv')
df.drop(columns=["Unnamed: 0"], inplace=True)
df.head()





from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier





# Make a copy of our data
train_df = df.copy()

# Separate target variable from independent variables
y = df['churn']
X = df.drop(columns=['id', 'churn'])
print(X.shape)
print(y.shape)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)





# Enhanced ML Pipeline with Comprehensive Error Handling
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, classification_report, confusion_matrix)
from sklearn.utils import resample
import logging

# Set up logging for error tracking
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class MLPipelineWithErrorHandling:
    """
    A robust ML pipeline with comprehensive error handling for churn prediction.
    """
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.model = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.best_balancing_method = None
        self.results = {}
        
    def safe_data_loading(self, filepath):
        """Safely load data with error handling"""
        try:
            df = pd.read_csv(filepath)
            logger.info(f"Successfully loaded data with shape: {df.shape}")
            
            # Handle common data issues
            if "Unnamed: 0" in df.columns:
                df = df.drop(columns=["Unnamed: 0"])
                logger.info("Removed unnamed index column")
                
            return df
            
        except FileNotFoundError:
            logger.error(f"File not found: {filepath}")
            raise
        except pd.errors.EmptyDataError:
            logger.error("CSV file is empty")
            raise
        except Exception as e:
            logger.error(f"Unexpected error loading data: {str(e)}")
            raise
    
    def safe_feature_target_split(self, df, target_col='churn', id_col='id'):
        """Safely split features and target with validation"""
        try:
            # Validate required columns exist
            if target_col not in df.columns:
                raise ValueError(f"Target column '{target_col}' not found in dataframe")
            
            # Prepare features and target
            cols_to_drop = [col for col in [id_col, target_col] if col in df.columns]
            X = df.drop(columns=cols_to_drop)
            y = df[target_col]
            
            # Validate data
            if X.empty or y.empty:
                raise ValueError("Features or target is empty")
                
            if X.isnull().any().any():
                logger.warning("Missing values found in features - consider handling them")
                
            logger.info(f"Features shape: {X.shape}, Target shape: {y.shape}")
            logger.info(f"Target distribution:\n{y.value_counts()}")
            
            return X, y
            
        except Exception as e:
            logger.error(f"Error in feature-target split: {str(e)}")
            raise
    
    def safe_train_test_split(self, X, y, test_size=0.25):
        """Safely perform train-test split with validation"""
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=self.random_state, 
                stratify=y
            )
            
            self.X_train, self.X_test = X_train, X_test
            self.y_train, self.y_test = y_train, y_test
            
            logger.info(f"Train set: {X_train.shape}, Test set: {X_test.shape}")
            logger.info(f"Train target distribution:\n{y_train.value_counts()}")
            
            return X_train, X_test, y_train, y_test
            
        except Exception as e:
            logger.error(f"Error in train-test split: {str(e)}")
            raise
    
    def manual_balance_data(self, X, y, method='oversample'):
        """
        Manual data balancing without imblearn dependency
        """
        try:
            df_combined = pd.concat([X, y], axis=1)
            
            if method == 'oversample':
                # Oversample minority class
                class_counts = y.value_counts()
                majority_class = class_counts.idxmax()
                minority_class = class_counts.idxmin()
                max_count = class_counts.max()
                
                majority_df = df_combined[df_combined[y.name] == majority_class]
                minority_df = df_combined[df_combined[y.name] == minority_class]
                
                # Oversample minority class
                minority_oversampled = resample(
                    minority_df, replace=True, n_samples=max_count, 
                    random_state=self.random_state
                )
                
                balanced_df = pd.concat([majority_df, minority_oversampled])
                
            elif method == 'undersample':
                # Undersample majority class
                class_counts = y.value_counts()
                majority_class = class_counts.idxmax()
                minority_class = class_counts.idxmin()
                min_count = class_counts.min()
                
                majority_df = df_combined[df_combined[y.name] == majority_class]
                minority_df = df_combined[df_combined[y.name] == minority_class]
                
                # Undersample majority class
                majority_undersampled = resample(
                    majority_df, replace=False, n_samples=min_count, 
                    random_state=self.random_state
                )
                
                balanced_df = pd.concat([majority_undersampled, minority_df])
            
            else:
                balanced_df = df_combined
            
            # Shuffle the balanced dataset
            balanced_df = balanced_df.sample(frac=1, random_state=self.random_state).reset_index(drop=True)
            
            X_balanced = balanced_df.drop(columns=[y.name])
            y_balanced = balanced_df[y.name]
            
            logger.info(f"Balanced data using {method}: {X_balanced.shape}")
            logger.info(f"New target distribution:\n{y_balanced.value_counts()}")
            
            return X_balanced, y_balanced
            
        except Exception as e:
            logger.error(f"Error in data balancing: {str(e)}")
            raise
    
    def compare_balancing_methods(self, X, y):
        """Compare different balancing methods"""
        methods = ['none', 'oversample', 'undersample']
        results = []
        
        for method in methods:
            try:
                logger.info(f"Testing balancing method: {method}")
                
                if method == 'none':
                    X_bal, y_bal = X, y
                else:
                    X_bal, y_bal = self.manual_balance_data(X, y, method)
                
                # Split balanced data
                X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(
                    X_bal, y_bal, test_size=0.2, random_state=self.random_state, 
                    stratify=y_bal
                )
                
                # Train model
                model = RandomForestClassifier(
                    random_state=self.random_state, 
                    n_estimators=100,  # Reduced for faster training
                    max_depth=10
                )
                model.fit(X_train_bal, y_train_bal)
                y_pred = model.predict(X_test_bal)
                
                # Calculate metrics
                metrics = {
                    'method': method,
                    'accuracy': accuracy_score(y_test_bal, y_pred),
                    'precision': precision_score(y_test_bal, y_pred, zero_division=0),
                    'recall': recall_score(y_test_bal, y_pred, zero_division=0),
                    'f1': f1_score(y_test_bal, y_pred, zero_division=0)
                }
                
                # Cross-validation
                try:
                    cv_scores = cross_val_score(model, X_bal, y_bal, cv=5, scoring='f1')
                    metrics['cv_f1_mean'] = np.mean(cv_scores)
                    metrics['cv_f1_std'] = np.std(cv_scores)
                except Exception as cv_e:
                    logger.warning(f"Cross-validation failed for {method}: {str(cv_e)}")
                    metrics['cv_f1_mean'] = metrics['f1']
                    metrics['cv_f1_std'] = 0
                
                results.append(metrics)
                logger.info(f"Method {method} - F1: {metrics['f1']:.4f}")
                
            except Exception as e:
                logger.error(f"Error testing method {method}: {str(e)}")
                continue
        
        if not results:
            raise ValueError("No balancing methods completed successfully")
            
        # Find best method
        results_df = pd.DataFrame(results)
        best_method = results_df.loc[results_df['cv_f1_mean'].idxmax(), 'method']
        self.best_balancing_method = best_method
        
        logger.info(f"Best balancing method: {best_method}")
        return results_df
    
    def train_final_model(self, X, y):
        """Train final model with best balancing method"""
        try:
            # Apply best balancing method
            if self.best_balancing_method == 'none':
                X_final, y_final = X, y
            else:
                X_final, y_final = self.manual_balance_data(X, y, self.best_balancing_method)
            
            # Final train-test split
            X_train, X_test, y_train, y_test = self.safe_train_test_split(X_final, y_final)
            
            # Train final model with optimized parameters
            self.model = RandomForestClassifier(
                random_state=self.random_state,
                n_estimators=200,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                class_weight='balanced'  # Additional balancing
            )
            
            logger.info("Training final Random Forest model...")
            self.model.fit(X_train, y_train)
            
            # Make predictions
            y_pred = self.model.predict(X_test)
            y_pred_proba = self.model.predict_proba(X_test)[:, 1]
            
            # Store results
            self.results = {
                'predictions': y_pred,
                'probabilities': y_pred_proba,
                'y_test': y_test,
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred),
                'recall': recall_score(y_test, y_pred),
                'f1': f1_score(y_test, y_pred),
                'confusion_matrix': confusion_matrix(y_test, y_pred),
                'classification_report': classification_report(y_test, y_pred)
            }
            
            logger.info("Final model training completed successfully!")
            return self.results
            
        except Exception as e:
            logger.error(f"Error in final model training: {str(e)}")
            raise
    
    def evaluate_model(self):
        """Comprehensive model evaluation"""
        if not self.results:
            raise ValueError("Model must be trained first")
        
        print("=== MODEL EVALUATION RESULTS ===")
        print(f"Accuracy:  {self.results['accuracy']:.4f}")
        print(f"Precision: {self.results['precision']:.4f}")
        print(f"Recall:    {self.results['recall']:.4f}")
        print(f"F1-Score:  {self.results['f1']:.4f}")
        print(f"Best Balancing Method: {self.best_balancing_method}")
        
        print("\n=== CONFUSION MATRIX ===")
        print(self.results['confusion_matrix'])
        
        print("\n=== DETAILED CLASSIFICATION REPORT ===")
        print(self.results['classification_report'])
        
        # Feature importance
        if hasattr(self.model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': self.X_train.columns,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            print("\n=== TOP 10 FEATURE IMPORTANCES ===")
            print(feature_importance.head(10))
        
        return self.results

# Usage Example
def run_ml_pipeline(data_path):
    """
    Complete ML pipeline execution with error handling
    """
    try:
        # Initialize pipeline
        pipeline = MLPipelineWithErrorHandling(random_state=42)
        
        # Load and prepare data
        df = pipeline.safe_data_loading(data_path)
        X, y = pipeline.safe_feature_target_split(df)
        
        # Compare balancing methods
        print("Comparing balancing methods...")
        balancing_results = pipeline.compare_balancing_methods(X, y)
        print("\nBalancing Method Comparison:")
        print(balancing_results.round(4))
        
        # Train final model
        print(f"\nTraining final model with {pipeline.best_balancing_method} balancing...")
        final_results = pipeline.train_final_model(X, y)
        
        # Evaluate model
        pipeline.evaluate_model()
        
        return pipeline, balancing_results, final_results
        
    except Exception as e:
        logger.error(f"Pipeline execution failed: {str(e)}")
        raise

# For your specific case - replace with your actual data path
if __name__ == "__main__":
    
    data_path = 'Documents/GitHub/BCG-Data-Science-Project/data_for_predictions.csv'
    
    try:
        pipeline, balancing_results, final_results = run_ml_pipeline(data_path)
        print("\n🎉 Pipeline completed successfully!")
        
    except Exception as e:
        print(f"❌ Pipeline failed: {str(e)}")
        print("Please check the data path and ensure all dependencies are installed.")

